import os
import shutil
import json
import gc
import re
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document 
from src.config import DATA_PATH, DB_PATH, EMBEDDING_MODEL, CHUNK_SIZE, CHUNK_OVERLAP

# --- è³‡æ–™æ¸…ç†å‡½å¼ ---
def clean_text_content(text):
    """æ¸…ç†æ–‡ä»¶å…§å®¹ï¼Œå»é™¤é ç¢¼ã€å¤šé¤˜ç©ºç™½èˆ‡é›œè¨Š"""
    if not isinstance(text, str): return ""
    text = re.sub(r'Page \d+ of \d+', '', text)
    text = re.sub(r'- \d+ -', '', text)
    text = re.sub(r'\n\s*\n', '\n\n', text)
    text = re.sub(r' +', ' ', text)
    return text.strip()

def create_vector_db():
    print("ğŸ“š æ­£åœ¨å»ºç«‹çŸ¥è­˜åº« (åŸå­åŒ– JSON + åˆ‡åˆ† PDF)...")
    
    if not os.path.exists(DATA_PATH):
        os.makedirs(DATA_PATH)
        print(f"âš ï¸ è«‹å°‡æª”æ¡ˆæ”¾å…¥ {DATA_PATH}")
        return

    # æœ€çµ‚è¦å¯«å…¥ DB çš„æ‰€æœ‰æ–‡ä»¶
    final_docs = []
    
    # ç”¨ä¾†åŒæ­¥å¯«å…¥ JSONL çš„åˆ—è¡¨ (çµ¦ BM25 ç”¨)
    jsonl_records = []

    # --- 1. è®€å– PDF (åˆ‡åˆ†ä¸¦æ¨™è¨» ID) ---
    pdf_files = [f for f in os.listdir(DATA_PATH) if f.endswith('.pdf')]
    if pdf_files:
        print(f"ğŸ“„ ç™¼ç¾ {len(pdf_files)} å€‹ PDFï¼Œè®€å–ä¸­...")
        loader = DirectoryLoader(DATA_PATH, glob="*.pdf", loader_cls=PyPDFLoader)
        try:
            raw_pdfs = loader.load()
            
            # æ¸…ç†æ–‡å­—
            for doc in raw_pdfs:
                doc.page_content = clean_text_content(doc.page_content)
            
            # åˆ‡åˆ†
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=CHUNK_SIZE,
                chunk_overlap=CHUNK_OVERLAP,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
            )
            pdf_chunks = text_splitter.split_documents(raw_pdfs)
            
            # ğŸ”¥ ç‚ºæ¯å€‹ PDF Chunk ç”Ÿæˆ ID
            for idx, chunk in enumerate(pdf_chunks):
                # ç”Ÿæˆ IDï¼špdf_æª”åé›œæ¹Š_åºè™Ÿ (é€™è£¡ç°¡åŒ–ç”¨ pdf_åºè™Ÿ)
                chunk_id = f"pdf#{idx:04d}"
                
                # æ›´æ–° metadata
                chunk.metadata["chunk_id"] = chunk_id
                chunk.metadata["source_type"] = "pdf"
                
                # â­ï¸ é—œéµï¼šå°‡ ID å¯«å…¥å…§å®¹ï¼Œè®“ Embedding åŒ…å« ID è³‡è¨Š
                # åŸå§‹å…§å®¹ä¿ç•™åœ¨ metadata ä»¥å‚™ä¸æ™‚ä¹‹éœ€
                original_text = chunk.page_content
                chunk.page_content = f"[{chunk_id}] {original_text}"
                
                final_docs.append(chunk)
                
                # æº–å‚™ JSONL ç´€éŒ„
                jsonl_records.append({
                    "id": chunk_id,
                    "text": original_text, # JSONL å­˜åŸå§‹æ–‡å­—ï¼Œserver.py æœƒè‡ªå·±åŠ  ID
                    "source": chunk.metadata.get("source", "pdf_doc")
                })
                
            print(f"   PDF è™•ç†å®Œæˆï¼Œå…± {len(pdf_chunks)} å€‹ç‰‡æ®µ")
            
        except Exception as e:
            print(f"âŒ è®€å– PDF éŒ¯èª¤: {e}")

    # --- 2. è®€å– JSON (åŸå­åŒ–è™•ç†ä¸¦æ¨™è¨» ID) ---
    json_files = [f for f in os.listdir(DATA_PATH) if f.endswith('.json')]
    for j_file in json_files:
        if j_file == "medical_docs_with_ids.jsonl": continue # è·³éè‡ªå·±ç”¢ç”Ÿçš„æª”æ¡ˆ

        print(f"ğŸ“‹ è™•ç† JSON: {j_file} ...")
        try:
            path = os.path.join(DATA_PATH, j_file)
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # çµ±ä¸€è½‰æˆ List
            knowledge_list = []
            if isinstance(data, dict) and "medical_knowledge" in data:
                knowledge_list = data["medical_knowledge"]
            elif isinstance(data, list):
                knowledge_list = data
            else:
                knowledge_list = [json.dumps(data, ensure_ascii=False)]

            # ğŸ”¥ åŸå­åŒ–ä¸¦ç”Ÿæˆ ID
            for idx, content in enumerate(knowledge_list):
                # å¦‚æœ content æ˜¯å­—å…¸ (ä¾‹å¦‚åŸæœ¬å°±æœ‰ id å’Œ text)ï¼Œå˜—è©¦æå–
                if isinstance(content, dict):
                    text_content = content.get("text") or content.get("content") or str(content)
                    # å¦‚æœåŸæª”æœ‰ ID å°±ç”¨ï¼Œæ²’æœ‰å°±ç”Ÿæˆ
                    c_id = content.get("id") or content.get("chunk_id") or f"med#{idx:04d}"
                else:
                    text_content = str(content)
                    c_id = f"med#{idx:04d}"

                clean_content = clean_text_content(text_content)
                
                if clean_content:
                    # â­ï¸ é—œéµï¼šå°‡ ID å¯«å…¥å…§å®¹
                    doc_content = f"[{c_id}] {clean_content}"
                    
                    doc = Document(
                        page_content=doc_content,
                        metadata={
                            "chunk_id": c_id,
                            "source": j_file, 
                            "type": "atomic_knowledge"
                        }
                    )
                    final_docs.append(doc)
                    
                    # æº–å‚™ JSONL ç´€éŒ„
                    jsonl_records.append({
                        "id": c_id,
                        "text": clean_content,
                        "source": j_file
                    })
                    
        except Exception as e:
            print(f"âŒ è®€å– JSON å¤±æ•— ({j_file}): {e}")

    # æª¢æŸ¥ç¸½æ•¸
    if not final_docs:
        print("âŒ æ²’æœ‰æœ‰æ•ˆè³‡æ–™å¯å¯«å…¥è³‡æ–™åº«ï¼")
        return

    print(f"ğŸ§© æœ€çµ‚å½™æ•´: å…± {len(final_docs)} å€‹çŸ¥è­˜ç‰‡æ®µ")

    # --- 3. ç”Ÿæˆ JSONL æª”æ¡ˆ (çµ¦ BM25 ç”¨) ---
    jsonl_output_path = os.path.join(DATA_PATH, "medical_docs_with_ids.jsonl")
    print(f"ğŸ’¾ æ­£åœ¨ç”¢ç”Ÿ BM25 ç”¨çš„ JSONL: {jsonl_output_path}")
    try:
        with open(jsonl_output_path, "w", encoding="utf-8") as f:
            for record in jsonl_records:
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
        print("âœ… JSONL æª”æ¡ˆå»ºç«‹å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ JSONL å»ºç«‹å¤±æ•—: {e}")

    # --- 4. æ¸…ç†èˆŠè³‡æ–™åº« ---
    gc.collect()
    if os.path.exists(DB_PATH):
        try:
            shutil.rmtree(DB_PATH)
            print("ğŸ—‘ï¸ å·²æ¸…é™¤èˆŠå‘é‡è³‡æ–™åº«")
        except:
            print("âš ï¸ ç„¡æ³•åˆªé™¤èˆŠè³‡æ–™åº«ï¼Œå˜—è©¦ç›´æ¥å¯«å…¥...")

    # --- 5. å¯«å…¥ Chroma ---
    print(f"ğŸš€ æ­£åœ¨å‘é‡åŒ–ä¸¦å¯«å…¥ (Model: {EMBEDDING_MODEL})...")
    embedding_func = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cuda', 'trust_remote_code': True}
    )
    
    # åˆ†æ‰¹å¯«å…¥
    batch_size = 5000
    for i in range(0, len(final_docs), batch_size):
        batch = final_docs[i:i+batch_size]
        Chroma.from_documents(
            documents=batch, 
            embedding=embedding_func, 
            persist_directory=DB_PATH
        )
        print(f"   å·²å¯«å…¥æ‰¹æ¬¡ {i} ~ {i+len(batch)}")
        
    print(f"ğŸ‰ çŸ¥è­˜åº«èˆ‡ç´¢å¼•æª”å»ºç«‹å®Œæˆï¼")

if __name__ == "__main__":
    create_vector_db()