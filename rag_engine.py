import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

import os
import json
import re

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    PromptTemplate,
    FewShotChatMessagePromptTemplate
)

from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_community.retrievers import BM25Retriever
from langchain.schema import Document

try:
    from src.config import DB_PATH, EMBEDDING_MODEL, LLM_MODEL, OLLAMA_NUM_CTX, DATA_PATH
except ImportError:
    DB_PATH = "./chroma_db"
    EMBEDDING_MODEL = "intfloat/multilingual-e5-large"
    LLM_MODEL = "llama3:8b"
    OLLAMA_NUM_CTX = 4096
    DATA_PATH = "./data"


_CACHED_LLM = None
_CACHED_VECTORDB = None
_CACHED_BM25 = None

def zh_char_tokenize(text: str):
    # å»æ‰æ‰€æœ‰ç©ºç™½
    text = re.sub(r"\s+", "", text)
    return list(text)


def load_resources():
    global _CACHED_LLM, _CACHED_VECTORDB, _CACHED_BM25

    if _CACHED_LLM is None:
        _CACHED_LLM = ChatOllama(
            model=LLM_MODEL,
            temperature=0.0,
            num_ctx=OLLAMA_NUM_CTX,
            num_gpu=-1
        )

    embedding_func = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cuda", "trust_remote_code": True}
    )

    #Chromaç¨ç«‹åˆå§‹åŒ–
    if _CACHED_VECTORDB is None:
        _CACHED_VECTORDB = Chroma(
            persist_directory=DB_PATH,
            embedding_function=embedding_func
        )

    #BM25å¾ medical_docs_with_ids.jsonl å»ºç«‹
    if _CACHED_BM25 is None:
        docs_for_bm25 = []
        jsonl_path = os.path.join(DATA_PATH, "medical_docs_with_ids.jsonl")

        if os.path.exists(jsonl_path):
            try:
                with open(jsonl_path, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if not line:
                            continue
                        obj = json.loads(line)
                        
                        # å–å¾— ID èˆ‡ Text
                        cid = obj.get("id") or obj.get("chunk_id") or obj.get("doc_id")
                        text = (obj.get("text") or obj.get("content") or obj.get("page_content") or "").strip()
                        
                        if cid and text:
                            #ä¿®æ”¹é‡é» 1ï¼šå°‡ ID å¯«å…¥å…§å®¹é–‹é ­ï¼Œè®“ LLM ã€Œçœ‹å¾—åˆ°ã€ID
                            content_with_id = f"[{cid}] {text}"
                            
                            docs_for_bm25.append(
                                Document(
                                    page_content=content_with_id, # é€™è£¡ä½¿ç”¨å¸¶æœ‰ ID çš„å…§å®¹
                                    metadata={"chunk_id": cid, "source": "medical_docs"}
                                )
                            )
            except Exception as e:
                print(f"âš ï¸ è­¦å‘Šï¼šè®€å– {jsonl_path} å¤±æ•—ï¼š{e}")
        else:
            print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° {jsonl_path}")
        
        if docs_for_bm25:
            _CACHED_BM25 = BM25Retriever.from_documents(
                docs_for_bm25,
                preprocess_func=zh_char_tokenize
            )
            _CACHED_BM25.k = 20
        else:
            print("âš ï¸ è­¦å‘Šï¼šBM25 ç„¡æ³•å»ºç«‹ï¼ˆjsonl ç„¡å…§å®¹æˆ–è®€å–å¤±æ•—ï¼‰")

    return _CACHED_LLM, _CACHED_VECTORDB, _CACHED_BM25


def get_qa_chain():
    llm, vectordb, bm25_retriever = load_resources()

    #è¨˜æ†¶æ”¹å¯« Prompt (Condense Question)
    condense_prompt = PromptTemplate.from_template(
    """è«‹æ ¹æ“šã€å°è©±æ­·å²ã€‘å°‡ç”¨æˆ¶çš„ã€å¾ŒçºŒè¿½å•ã€‘æ”¹å¯«æˆä¸€å€‹ç¨ç«‹ã€å®Œæ•´çš„æœå°‹å•é¡Œã€‚

    è¦æ±‚ï¼š
    1. å¦‚æœè¿½å•åŒ…å«ä»£åè©ï¼ˆå¦‚ã€Œå®ƒã€ã€ã€Œé€™ç¨®ç—…ã€ï¼‰ï¼Œè«‹æ›¿æ›æˆæ­·å²å°è©±ä¸­çš„å…·é«”åè©ã€‚
    2. ä¿ç•™å°ˆæœ‰åè©ï¼ˆå¦‚ã€ŒWå‹åå§¿ã€ã€ã€ŒHVAã€ï¼‰ã€‚
    3. ä¸è¦å›ç­”å•é¡Œï¼Œåªè¦æ”¹å¯«å•é¡Œã€‚

    å°è©±æ­·å²ï¼š
    {chat_history}

    å¾ŒçºŒè¿½å•ï¼š{question}
    ç¨ç«‹å•é¡Œï¼š"""
    )

    #ä½¿ç”¨ç¯„ä¾‹å¼•å°å›ç­”
    examples = [
        {
            "question": "æ‹‡è¶¾å¤–ç¿»çš„å®šç¾©æ˜¯ä»€éº¼ï¼Ÿ",
            "answer": "æ‹‡è¶¾å¤–ç¿»æ˜¯æŒ‡ç¬¬ä¸€æŒéª¨èˆ‡å¤§æ‹‡è¶¾çš„é—œç¯€å¤–å‡¸è®Šå½¢ [med#0003]ã€‚éºå‚³æ˜¯æœ€å¤§çš„æˆå›  [med#0004]ã€‚"
        }
    ]

    example_prompt = ChatPromptTemplate.from_messages(
        [("human", "{question}"), ("ai", "{answer}")]
    )
    few_shot_prompt = FewShotChatMessagePromptTemplate(
        example_prompt=example_prompt,
        examples=examples,
    )

    # Prompt
    system_template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è¶³éƒ¨é†«å­¸å°ˆå®¶ã€‚è«‹é‡å°ç”¨æˆ¶å•é¡Œæä¾›ã€ç²¾ç¢ºä½†å®Œæ•´ã€‘çš„å›ç­”ã€‚
    
    **é‡é»**ï¼šä»¥ä¸‹æç¤ºè©åªæœ‰æˆ‘å€‘äº’ç›¸çŸ¥é“ï¼Œ**ä¸è¦**å°‡æç¤ºè©çš„è©å‡ºç¾åœ¨å›ç­”ä¸­ã€‚

    1.**è­˜åˆ¥å•é¡Œé¡å‹**
        - æ¦‚å¿µè§£é‡‹é¡ï¼šæä¾›æ¸…æ™°å®šç¾© â†’ èˆ‰ä¾‹èªªæ˜ â†’ å»¶ä¼¸æ‡‰ç”¨
        - æ“ä½œæŒ‡å°é¡ï¼šç°¡è¿°ç›®æ¨™ â†’ æ­¥é©Ÿèªªæ˜ â†’ æ³¨æ„äº‹é …
        - æ¯”è¼ƒåˆ†æé¡ï¼šåˆ—å‡ºå°è±¡ â†’ é—œéµå·®ç•° â†’ é¸æ“‡å»ºè­°
        - ç„¡é—œå•é¡Œï¼šç¦®è²Œå›æ‡‰ä¸¦å¼•å°å›ä¸»é¡Œ


    2. **å®Œæ•´å¥å‹**ï¼šå›ç­”çš„é¦–å¥**å¿…é ˆ**åŒ…å«å•é¡Œçš„é—œéµå­—æˆ–ä¸»è©ã€‚
    
    3. å¦‚æœæ–‡ä»¶ä¸­æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œç›´æ¥å›ç­”ã€Œè³‡æ–™ä¸è¶³ã€ã€‚

    4. **è³‡è¨Šæ•´åˆ**ï¼šè«‹å°‡åˆ†æ•£åœ¨ä¸åŒæ®µè½çš„ç›¸é—œè³‡è¨Šï¼Œæ‹¼æ¹Šæˆå®Œæ•´ç­”æ¡ˆã€‚

    5. **å®‰å…¨èˆ‡é™åˆ¶**ï¼š
       - **åš´ç¦æ¨éŠ·**ï¼šä¸æä¾›å•†å“çš„æ¨è–¦æˆ–éŠ·å”®è³‡è¨Šã€‚
       - **é†«ç™‚å…è²¬**ï¼šå¦‚æ¶‰åŠåš´é‡ç—‡ç‹€ï¼Œå»ºè­°å°‹æ±‚å°ˆæ¥­é†«ç™‚å”åŠ©ã€‚

    6. **æ ¼å¼è¦æ±‚**ï¼š
       - ä½¿ç”¨ç²—é«” (**é—œéµå­—**) æ¨™ç¤ºé‡é»ã€‚
       - ä½¿ç”¨æ¢åˆ—å¼æ¸…å–®ã€‚
       - **åš´ç¦**ä½¿ç”¨ Markdown æ¨™é¡Œç¬¦è™Ÿ (#, ##)ã€‚

    7. **ğŸŒ èªè¨€é¡åƒè¦å‰‡**ï¼š
       - è‹¥ç”¨æˆ¶ç”¨è‹±æ–‡å•ï¼Œå¿…é ˆç”¨è‹±æ–‡å›ç­”ã€‚
       - è‹¥ç”¨æˆ¶ç”¨ä¸­æ–‡å•ï¼Œå¿…é ˆç”¨ **å°ç£ç¹é«”ä¸­æ–‡** å›ç­”ã€‚

    8. ğŸ”¥ **å¼•ç”¨è¦å‰‡ (å¿…é ˆåš´æ ¼éµå®ˆ)**ï¼š
       - **æ¯ä¸€å¥è©±**æˆ–**æ¯ä¸€å€‹è«–é»**çš„çµå°¾ï¼Œéƒ½**å¿…é ˆ**åŠ ä¸Šä¾†æºæ–‡ä»¶çš„ IDã€‚
       - ID çš„æ ¼å¼å¿…é ˆæ˜¯ **[med#xxxx]**ã€‚
       - å¦‚æœä¸€å¥è©±ç¶œåˆäº†å¤šå€‹æ–‡ä»¶çš„è³‡è¨Šï¼Œè«‹æ¨™è¨»æ‰€æœ‰ç›¸é—œ IDï¼Œä¾‹å¦‚ï¼š[med#0001][med#0005]ã€‚

    9. ğŸ§  **èªæ„å®¹éŒ¯æ©Ÿåˆ¶**ï¼š
       - ç”¨æˆ¶å¸¸å°‡ã€Œæ‹‡è¶¾ã€ï¼ˆè…³ï¼‰èª¤æ‰“ç‚ºã€Œæ‹‡æŒ‡ã€ï¼ˆæ‰‹ï¼‰ï¼Œè«‹è¦–ç‚ºç›¸åŒæ¦‚å¿µè™•ç†ã€‚
       - ç”¨æˆ¶å¸¸å°‡ã€Œè¶³åº•ç­‹è†œç‚ã€ç°¡ç¨±ç‚ºã€Œç­‹è†œç‚ã€ï¼Œè«‹è¦–ç‚ºç›¸åŒæ¦‚å¿µè™•ç†ã€‚

    ã€åƒè€ƒæ–‡ä»¶ã€‘ï¼š
    {context}
    """
    
    #ç¨ç«‹å®šç¾© human_template è®Šæ•¸
    human_template = "{question}"

    qa_prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(system_template),
        few_shot_prompt,
        HumanMessagePromptTemplate.from_template(human_template),
    ])

    #Retriever è¨­å®š
    vector_retriever = vectordb.as_retriever(search_kwargs={"k": 20})

    if bm25_retriever:
        base_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, vector_retriever],
            weights=[0.5, 0.5]
        )
    else:
        base_retriever = vector_retriever

    #Reranker è¨­å®š
    reranker_model = HuggingFaceCrossEncoder(
        model_name="BAAI/bge-reranker-large",
        model_kwargs={"device": "cuda"}
    )
    compressor = CrossEncoderReranker(model=reranker_model, top_n=8)

    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=base_retriever
    )

    # å»ºç«‹Chain
    # ä½¿ç”¨return_source_documents=True,åœ¨Server ç«¯çš„ Debug å°å‡º ID
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=compression_retriever,
        condense_question_prompt=condense_prompt,
        combine_docs_chain_kwargs={"prompt": qa_prompt},
        return_source_documents=True 
    )

    return chain